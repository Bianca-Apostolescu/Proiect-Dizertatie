{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "83DJlE1Mst43",
        "gwVB4g9VrwN-",
        "B_Bh6FWJu_Sl",
        "iAe84x2v3J8F",
        "axY34Rg32kyd"
      ],
      "authorship_tag": "ABX9TyOomqGAQcs/Q0sflI+rHTEk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bianca-Apostolescu/Proiect-Dizertatie/blob/interm/People_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install segmentation_models_pytorch"
      ],
      "metadata": {
        "id": "s8ijPGsW3cLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iniatial"
      ],
      "metadata": {
        "id": "83DJlE1Mst43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Master\n",
        "!mkdir /content/Master/instances\n",
        "!mkdir /content/Master/Output\n",
        "\n",
        "!wget -P Master http://images.cocodataset.org/zips/val2017.zip\n",
        "!wget -P Master http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip\n",
        "!wget -P /content/Master/instances http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "# !wget -P Master http://images.cocodataset.org/zips/train2017.zip"
      ],
      "metadata": {
        "id": "Aq0p0-m7ruYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Master/val2017.zip -d Master\n",
        "!unzip /content/Master/panoptic_annotations_trainval2017.zip -d Master\n",
        "!unzip /content/Master/instances/annotations_trainval2017.zip -d /content/Master/instances\n",
        "# !unzip /content/Master/train2017.zip -d Master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYyAo9ufsw3m",
        "outputId": "97d1e88c-2911-4f8b-ca3b-22985efbf964"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Master/instances/annotations_trainval2017.zip\n",
            "  inflating: /content/Master/instances/annotations/instances_train2017.json  \n",
            "  inflating: /content/Master/instances/annotations/instances_val2017.json  \n",
            "  inflating: /content/Master/instances/annotations/captions_train2017.json  \n",
            "  inflating: /content/Master/instances/annotations/captions_val2017.json  \n",
            "  inflating: /content/Master/instances/annotations/person_keypoints_train2017.json  \n",
            "  inflating: /content/Master/instances/annotations/person_keypoints_val2017.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Master/annotations/panoptic_val2017.zip -d Master\n",
        "!mv /content/Master/annotations/panoptic_val2017.json /content/Master # move src dest\n",
        "!mv /content/Master/instances/annotations/instances_val2017.json /content/Master # move src dest\n",
        "# !unzip /content/Master/panoptic_annotations_trainval2017.zip -d Master"
      ],
      "metadata": {
        "id": "U8uRR6EQvkfu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/Master/val2017.zip\n",
        "!rm -r /content/Master/panoptic_annotations_trainval2017.zip\n",
        "# !rm -r /content/Master/train2017.zip\n",
        "\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/Master/annotations', ignore_errors=True)\n",
        "shutil.rmtree('/content/Master/instances', ignore_errors=True)\n",
        "shutil.rmtree('/content/Master/__MACOSX', ignore_errors=True)"
      ],
      "metadata": {
        "id": "oVVZBLe7sw6F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/Master/annotations', ignore_errors=True)\n",
        "shutil.rmtree('/content/Master/instances', ignore_errors=True)"
      ],
      "metadata": {
        "id": "b2mXVtIysw7-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AFg1pMQrrua1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "gwVB4g9VrwN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For managing COCO dataset\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# For creating and managing folder/ files\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# For managing images\n",
        "from PIL import Image\n",
        "import skimage.io as io\n",
        "\n",
        "# Basic libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "\n",
        "# For importing models and working with them\n",
        "## Torch\n",
        "import torch\n",
        "import torch.utils.data # for Dataset\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "# For creating train - test splits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pathlib\n",
        "import pylab\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from imutils import paths\n",
        "\n",
        "# Performance Metrics\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "\n",
        "# %matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D_2fj96rudO",
        "outputId": "9968d7b9-faca-4213-9c4e-58c3e4d768c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "B_Bh6FWJu_Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = 'val2017'\n",
        "\n",
        "# Data and Masks\n",
        "IMAGE_PATH = os.path.join('/content/Master/', data)\n",
        "MASK_PATH = os.path.join('/content/Master/', 'panoptic_' + data)\n",
        "\n",
        "# define the path to the base output directory\n",
        "BASE_OUTPUT = '/content/Master/Output'\n",
        "\n",
        "# define the path to the output serialized model, model training plot, and testing image paths\n",
        "MODEL_PATH = os.path.join(BASE_OUTPUT, \"unet_tgs_salt.pth\")\n",
        "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
        "TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])\n",
        "\n",
        "\n",
        "\n",
        "# determine if we will be pinning memory during data loading\n",
        "PIN_MEMORY = True if device == \"cuda\" else False\n",
        "\n",
        "\n",
        "# define the number of channels in the input, number of classes, and number of levels in the U-Net model\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2 # person vs Background\n",
        "\n",
        "# define the input image dimensions\n",
        "INPUT_IMAGE_WIDTH = 256\n",
        "INPUT_IMAGE_HEIGHT = 256\n",
        "\n",
        "# define threshold to filter weak predictions\n",
        "THRESHOLD = 0.5 # for binary classification"
      ],
      "metadata": {
        "id": "0WAibjCQu-xv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "JYc7ePOfr6bV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "siRJBwYOxvt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, imagePaths, maskPaths, transforms):\n",
        "        # store the image and mask filepaths, and augmentation transforms\n",
        "        self.imagePaths = imagePaths\n",
        "        self.maskPaths = maskPaths\n",
        "        self.transforms = transforms\n",
        "        self.image_list = []\n",
        "        self.mask_list = []\n",
        "\n",
        "        # Loading Images\n",
        "\n",
        "        for index in range (0, len(imagePaths)):\n",
        "          # grab the image path from the current index\n",
        "          imagePath = self.imagePaths[index]\n",
        "          \n",
        "          # load the image from disk, swap its channels from BGR to RGB, and read the associated mask from disk in grayscale mode\n",
        "          image = cv2.imread(imagePath)\n",
        "          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "          mask = cv2.imread(self.maskPaths[index]) #, 0)\n",
        "\n",
        "          if self.transforms is not None:\n",
        "            # apply the transformations to both image and its mask\n",
        "            image = self.transforms(image)\n",
        "            mask = self.transforms(mask)\n",
        "\n",
        "          self.image_list.append(image)\n",
        "          self.mask_list.append(mask)\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        # return the number of total samples contained in the dataset\n",
        "        return len(self.imagePaths)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index): #idx\n",
        "        # # grab the image path from the current index\n",
        "        # imagePath = self.imagePaths[idx]\n",
        "        \n",
        "        # # load the image from disk, swap its channels from BGR to RGB, and read the associated mask from disk in grayscale mode\n",
        "        # image = cv2.imread(imagePath)\n",
        "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        # mask = cv2.imread(self.maskPaths[idx]) #, 0)\n",
        "        \n",
        "        # check to see if we are applying any transformations\n",
        "        # if self.transforms is not None:\n",
        "        #     # apply the transformations to both image and its mask\n",
        "        #     image = self.transforms(image)\n",
        "        #     mask = self.transforms(mask)\n",
        "            \n",
        "        # return a tuple of the image and its mask\n",
        "        return (self.image_list[index], self.mask_list[index])"
      ],
      "metadata": {
        "id": "1SWKHlF3rulI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FavXUSsXxzWl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cxx2-muvxzY5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mask = cv2.imread(trainMasks[0], 0)\n",
        "# mask2 = cv2.imread(trainMasks[0])\n",
        "# # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "# plt.imshow(mask)\n"
      ],
      "metadata": {
        "id": "4iM1yopwruoF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(mask2)\n",
        "# mask2.shape"
      ],
      "metadata": {
        "id": "l61zkc6T74af"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "F2mGv5N9urEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "iAe84x2v3J8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load COCO annotations \n",
        "ann = '/content/Master/instances_val2017.json'\n",
        "\n",
        "# Initialize the COCO api for instance annotations\n",
        "coco = COCO(ann)\n",
        "\n",
        "# Define the classes (out of the 81) which you want to see. Others will not be shown - just Person\n",
        "filterClasses = ['person']\n",
        "\n",
        "# Fetch class IDs only corresponding to the filterClasses\n",
        "catIds = coco.getCatIds(catNms = filterClasses) \n",
        "\n",
        "# Get all images containing the above Category IDs\n",
        "imgIds = coco.getImgIds(catIds = catIds)\n",
        "print(\"Number of images containing all the  classes:\", len(imgIds))\n",
        "\n",
        "\n",
        "# Lists of Person Images and Masks\n",
        "arr_images = []\n",
        "arr_masks = []\n",
        "for i in range (0, len(imgIds)):\n",
        "    img = coco.loadImgs(imgIds[i])[0]\n",
        "    arr_images.append((IMAGE_PATH + '/' + img['file_name']))\n",
        "    arr_masks.append((MASK_PATH + '/' + img['file_name'][:12] + '.png'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWurmkptruqa",
        "outputId": "cf5f3eec-4fd3-4fc6-fc19-35748f21e981"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.04s)\n",
            "creating index...\n",
            "index created!\n",
            "Number of images containing all the  classes: 2693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AnMM2XDLyZDG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists of images and masks \n",
        "imagePaths = arr_images\n",
        "maskPaths = arr_masks\n",
        "\n",
        "# Split data into train and test\n",
        "TEST_SPLIT = 0.3\n",
        "\n",
        "split = train_test_split(imagePaths, maskPaths, test_size = TEST_SPLIT, random_state = 19)\n",
        "\n",
        "# Unpack the data split\n",
        "(trainImages, testImages) = split[:2]\n",
        "(trainMasks, testMasks) = split[2:]\n",
        "\n",
        "# Write the testing image paths to disk so that we can use then when evaluating/testing our model\n",
        "print(\"[INFO] saving testing image paths...\")\n",
        "\n",
        "f = open(TEST_PATHS, \"w\")\n",
        "f.write(\"\\n\".join(testImages))\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbaVFr_MyZIO",
        "outputId": "1c90064a-2446-4165-e28f-da5cd082ae26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] saving testing image paths...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWXZWJ2gyZKi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets and Dataloaders"
      ],
      "metadata": {
        "id": "axY34Rg32kyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize training parameters\n",
        "\n",
        "# TEST_SPLIT = 0.3\n",
        "\n",
        "INIT_LR = 0.001\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 16 "
      ],
      "metadata": {
        "id": "oMCZEtxJ2zNF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define transformations\n",
        "transforms_train = torchvision.transforms.Compose([transforms.ToPILImage(),\n",
        "                                                   transforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH)),\n",
        "                                                   transforms.ToTensor(),\n",
        "                                                   transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
        "                                                                        std = [0.229, 0.224, 0.225])\n",
        "                                                  ])\n",
        "\n",
        "transforms_test = torchvision.transforms.Compose([transforms.ToPILImage(),\n",
        "                                                  transforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH)),\n",
        "                                                  transforms.ToTensor()\n",
        "                                                 ])\n",
        "\n",
        "# create the train and test datasets\n",
        "trainDS = SegmentationDataset(imagePaths = trainImages, maskPaths = trainMasks, transforms = transforms_train)\n",
        "testDS = SegmentationDataset(imagePaths = testImages, maskPaths = testMasks, transforms = transforms_test)\n",
        "\n",
        "\n",
        "print(f\"[INFO] found {len(trainDS)} examples in the training set...\")\n",
        "print(f\"[INFO] found {len(testDS)} examples in the test set...\")\n",
        "\n",
        "\n",
        "# create the training and test data loaders\n",
        "trainLoader = DataLoader(trainDS, \n",
        "                         shuffle = True,\n",
        "                         batch_size = BATCH_SIZE, \n",
        "                         pin_memory = PIN_MEMORY\n",
        "#                          num_workers = os.cpu_count()\n",
        "                        )\n",
        "testLoader = DataLoader(testDS, \n",
        "                        shuffle = False,\n",
        "                        batch_size = BATCH_SIZE, \n",
        "                        pin_memory = PIN_MEMORY\n",
        "#                         num_workers = os.cpu_count()\n",
        "                       )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FphPQyiKyZM9",
        "outputId": "9afc93dd-2fbd-4b60-c29a-8fec63c12e17"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] found 1885 examples in the training set...\n",
            "[INFO] found 808 examples in the test set...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z_VbELDTyZWu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Model"
      ],
      "metadata": {
        "id": "6wm0BWIzw-Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/qubvel/segmentation_models.pytorch\n",
        "# https://github.com/qubvel/segmentation_models.pytorch#architectures\n",
        "    \n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "model = smp.Unet(\n",
        "                    encoder_name = \"resnet34\",                    # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
        "                    encoder_weights = \"imagenet\",                 # use `imagenet` pre-trained weights for encoder initialization\n",
        "                    in_channels = NUM_CHANNELS,                   # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
        "                    classes = NUM_CHANNELS,                        # model output channels (number of classes in your dataset) - person vs bg\n",
        "                )"
      ],
      "metadata": {
        "id": "Urce4MkCrus9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize loss function and optimizer\n",
        "lossFunc = nn.BCEWithLogitsLoss()\n",
        "opt = Adam(model.parameters(), lr = INIT_LR)\n",
        "\n",
        "# calculate steps per epoch for training and test set\n",
        "trainSteps = len(trainDS) // BATCH_SIZE\n",
        "testSteps = len(testDS) // BATCH_SIZE\n",
        "\n",
        "# initialize a dictionary to store training history\n",
        "H = {\"train_loss\": [], \"test_loss\": []}"
      ],
      "metadata": {
        "id": "VHRE4cUKutCl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDHyCHaCutFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "mYvox99E4YMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over epochs\n",
        "print(\"[INFO] training the network...\")\n",
        "\n",
        "startTime = time.time()\n",
        "\n",
        "for e in tqdm(range(NUM_EPOCHS)):\n",
        "    \n",
        "    # set the model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    # initialize the total training and validation loss\n",
        "    totalTrainLoss = 0\n",
        "    totalTestLoss = 0\n",
        "#     c = 0\n",
        "    \n",
        "    # loop over the training set\n",
        "    for (i, (x, y)) in enumerate(trainLoader):\n",
        "#         print(c + 1)\n",
        "        \n",
        "        # send the input to the device\n",
        "        (x, y) = (x.to(device), y.to(device))\n",
        "        \n",
        "        # perform a forward pass and calculate the training loss\n",
        "        pred = model(x)\n",
        "        loss = lossFunc(pred, y)\n",
        "        \n",
        "        # first, zero out any previously accumulated gradients, then\n",
        "        # perform backpropagation, and then update model parameters\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        \n",
        "        # add the loss to the total training loss so far\n",
        "        totalTrainLoss += loss\n",
        "        \n",
        "#         c = c + 1\n",
        "        \n",
        "    # switch off autograd\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        # set the model in evaluation mode\n",
        "        model.eval()\n",
        "        \n",
        "        # loop over the validation set\n",
        "        for (x, y) in testLoader:\n",
        "            # send the input to the device\n",
        "            (x, y) = (x.to(device), y.to(device))\n",
        "            \n",
        "            # make the predictions and calculate the validation loss\n",
        "            pred = model(x)\n",
        "            totalTestLoss += lossFunc(pred, y)\n",
        "            \n",
        "    # calculate the average training and validation loss\n",
        "    avgTrainLoss = totalTrainLoss / trainSteps\n",
        "    avgTestLoss = totalTestLoss / testSteps\n",
        "    \n",
        "    # update our training history\n",
        "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "    H[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
        "    \n",
        "    # print the model training and validation information\n",
        "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
        "    print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(avgTrainLoss, avgTestLoss))\n",
        "    \n",
        "    \n",
        "# display the total time needed to perform the training\n",
        "endTime = time.time()\n",
        "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "c7ljfePzutHn",
        "outputId": "0a7e3834-66aa-4479-d2ce-0d1bde1c3477"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training the network...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [03:48<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5ba987bb3959>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# perform backpropagation, and then update model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tq0zsiuoutKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(i, (x, y)) = next(enumerate(trainLoader))"
      ],
      "metadata": {
        "id": "VgXVKU3MutMs"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXQJty-WutPE",
        "outputId": "6b1b1871-aff5-4f6a-cf14-1b4fa7fa74fc"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    }
  ]
}